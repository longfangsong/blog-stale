---
title: 信息论知识点整理
date: 2020-04-17 00:00:00
tags: [信息论]
mathjax: true
comments: true
summary: 信息论基本公式和定义的整理
---
## 定义

###（自）信息量
随机事件的自信息量定义为其发生概率的负对数。
$$
I(x) = -log(P(x))
$$

### 互信息量
某事件的发生给出的关于另一事件是否发生的信息量。
$$
I(x;y) = I(x)-I(x|y)=log(P(x|y)-P(x))
$$
表示已知$y$时消除的$x$的不确定性。

### 平均自信息量（信息熵）
随机变量$X$所有取值$x$的自信息量的平均值。

一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以，信息熵也可以说是系统有序化程度的一个度量。
$$
H(X) = E[I(x)] = -\sum_i P(x_i)log(P(x_i))
$$

### 联合熵

联合自信息的数学期望。
$$
H(XY) = \sum_i \sum_j P(x_iy_j) log\ P(x_iy_j)
$$

### 条件熵

已知随机变量 $X$ 给定时，$Y$ 带来的熵。
$$
H(Y | X) = \sum_iP(x_i)H(Y|x_i)=-\sum_i \sum_j P(x_i)P(y_j|x_i)log\ P(y_j|x_i)
$$

### 平均互信息量

$$
I(X;Y) = \sum_i \sum_j P(x_iy_j)I(x_i;y_j)=H(X)-H(X|Y)
$$

### 平均条件互信息

表示随机变量$Z$给定后，从随机变量$Y$所得到的关于随机变量$X$的信息量。
$$
I(X;Y|Z)=E\{I(xy|z)\}=\sum_{x,y,z}P(xyz)log(\frac {P(x|yz)} {P(x|z)})
$$

### 平均联合互信息

表示从二维随机变量$YZ$所得到的关于随机变量$X$的信息量。
$$
I(X;YZ)=E\{I(x;yz)\}=\sum_{x,y,z}P(xyz)log(\frac {P(x|yz)} {P(x)})
$$

### 信源

信息的来源，产生消息（符号）、时间离散的消息序列（符号序列）以及时间连续的消息的来源。

#### 离散单符号信源

输出单个离散取值的符号的信源。

#### 离散多符号信源

##### 马尔可夫信源

某时刻发出某的符号的概率仅和之前发生的有限个符号有关。其中的“有限个”的个数就是马尔可夫信源的阶数。



### 信源的相对率

一个信源的熵率（极限熵）与具有相同符号集的最大熵的比。
$$
\eta = \frac {H_\infty} {H_0}
$$

### 信源的剩余度

$$
\gamma = 1-\eta=1-\frac {H_\infty} {H_0}=1-\frac {H_\infty} {log\ q}
$$



## 性质

### 信息熵

- 对称性
- 确定性
- 非负性
- 扩展性
- 连续性
- 递增性
- 极值性
- 上凸性

### 平均互信息

- 非负性
- 对称性
- 极值性
- 凸函数性

## 定理

如果随机变量$X,Y,Z$构成一个马尔可夫链，则有以下关系成立：
$$
I(X;Z) \le I(X;Y), I(X,Z) \le I(Y;Z)
$$
在$P(x|yz)=P(x|z)\ P(z|xy)=P(z|x)$时取等号。



## 计算公式

1. 熵函数的链式法则
   $$
   H(X_1X_2...X_N)=H(X_1)+H(X_2|X_1)+...+H(X_N|X_1X_2...X_{N-1})
   $$

2. 平均互信息
   $$
   I(X;Y)=H(X)-H(X|Y)\\
   =H(Y)-H(Y|X)\\
   =H(X)+H(Y)-H(XY)
   $$
   当$X,Y$独立时，$I(X;Y)=0$